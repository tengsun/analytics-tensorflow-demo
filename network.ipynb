{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before training weights:\n",
      "[[-0.8113182   1.4845988   0.06532937]\n",
      " [-2.4427042   0.0992484   0.5912243 ]]\n",
      "[[-0.8113182 ]\n",
      " [ 1.4845988 ]\n",
      " [ 0.06532937]]\n",
      "after 0 training steps, cross entroy is 1.89805\n",
      "after 1000 training steps, cross entroy is 0.655075\n",
      "after 2000 training steps, cross entroy is 0.626172\n",
      "after 3000 training steps, cross entroy is 0.615096\n",
      "after 4000 training steps, cross entroy is 0.610309\n",
      "after training weights:\n",
      "[[ 0.02476983  0.56948674  1.6921941 ]\n",
      " [-2.1977348  -0.23668921  1.1143895 ]]\n",
      "[[-0.45544702]\n",
      " [ 0.49110925]\n",
      " [-0.98110336]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# STEP 1: define the neural network topology\n",
    "# define training batch size\n",
    "batch_size = 8\n",
    "\n",
    "# use seed to make sure every time got the same w1, w2\n",
    "w1 = tf.Variable(tf.random_normal([2, 3], stddev=1, seed=1))\n",
    "w2 = tf.Variable(tf.random_normal([3, 1], stddev=1, seed=1))\n",
    "\n",
    "# use placeholder instead of constant to reduce resource consumption\n",
    "# use None dimension for row to adjust the size of training dataset\n",
    "x = tf.placeholder(tf.float32, shape=(None, 2), name='x-input')\n",
    "y_ = tf.placeholder(tf.float32, shape=(None, 1), name='y-input')\n",
    "# x = tf.constant([[0.7, 0.9]])\n",
    "\n",
    "# network topology: x-(w1)-a-(w2)-y\n",
    "a = tf.matmul(x, w1)\n",
    "y = tf.matmul(a, w2)\n",
    "\n",
    "\n",
    "# STEP 2: define loss function and back propagation algorithm\n",
    "y = tf.sigmoid(y)\n",
    "cross_entropy = -tf.reduce_mean(\n",
    "    y_ * tf.log(tf.clip_by_value(y, 1e-10, 1.0))\n",
    "    + (1-y_) * tf.log(tf.clip_by_value(1-y, 1e-10, 1.0)))\n",
    "train_step = tf.train.AdamOptimizer(0.001).minimize(cross_entropy)\n",
    "\n",
    "\n",
    "# STEP 3: define the session to execute\n",
    "# generate a random test dataset\n",
    "rdm = np.random.RandomState(1)\n",
    "dataset_size = 128\n",
    "X = rdm.rand(dataset_size, 2)\n",
    "# labeling 1 as positive, 0 as negative\n",
    "Y = [[int(x1+x2 < 1)] for (x1,x2) in X]\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # use global variables initialization\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    # sess.run(w1.initializer)\n",
    "    # sess.run(w2.initializer)\n",
    "    \n",
    "    print('before training weights:')\n",
    "    print(sess.run(w1))\n",
    "    print(sess.run(w2))\n",
    "    \n",
    "    # set training times\n",
    "    STEPS = 5000\n",
    "    for i in range(STEPS):\n",
    "        # pick data with batch_size to train\n",
    "        start = (i * batch_size) % dataset_size\n",
    "        end = min(start + batch_size, dataset_size)\n",
    "        \n",
    "        # train neural network and adjust weights\n",
    "        sess.run(train_step, feed_dict={x:X[start:end], y_:Y[start:end]})\n",
    "        if i % 1000 == 0:\n",
    "            total_cross_entropy = sess.run(cross_entropy, feed_dict={x:X, y_:Y})\n",
    "            print('after %d training steps, cross entroy is %g' % (i, total_cross_entropy))\n",
    "    \n",
    "    print('after training weights:')\n",
    "    print(sess.run(w1))\n",
    "    print(sess.run(w2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
