{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets('./data/mnist/', one_hot=True)\n",
    "\n",
    "print('train data size:', mnist.train.num_examples)\n",
    "print('validation data size:', mnist.validation.num_examples)\n",
    "print('test data size:', mnist.test.num_examples)\n",
    "\n",
    "print('example train data:', mnist.train.images[0])\n",
    "print('example train label:', mnist.train.labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting ./data/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting ./data/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./data/mnist/t10k-labels-idx1-ubyte.gz\n",
      "after 0 training steps, validation accuracy is 0.1454 \n",
      "after 1000 training steps, validation accuracy is 0.1446 \n",
      "after 2000 training steps, validation accuracy is 0.1448 \n",
      "after 3000 training steps, validation accuracy is 0.1448 \n",
      "after 4000 training steps, validation accuracy is 0.1452 \n",
      "after 5000 training steps, validation accuracy is 0.1452 \n",
      "after 6000 training steps, validation accuracy is 0.1444 \n",
      "after 7000 training steps, validation accuracy is 0.142 \n",
      "after 8000 training steps, validation accuracy is 0.1408 \n",
      "after 9000 training steps, validation accuracy is 0.14 \n",
      "after 10000 training steps, test accuracy is 0.1427 \n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# data set params (28x28, 1-10)\n",
    "INPUT_NODE = 784\n",
    "OUTPUT_NODE = 10\n",
    "\n",
    "# neural network params\n",
    "LAYER1_NODE = 500\n",
    "BATCH_SIZE = 100\n",
    "LEARNING_RATE_BASE = 0.8\n",
    "LEARNING_RATE_DECAY = 0.99\n",
    "REGULARIZATION_RATE = 0.0001\n",
    "TRAINING_STEPS = 10000\n",
    "MOVING_AVERAGE_DECAY = 0.99\n",
    "\n",
    "# calculate forward propagation result with ReLU activation function\n",
    "def inference(input_tensor, avg_class, weights1, biases1, weights2, biases2):\n",
    "    # if avg_class is None, use the weights and biases directly\n",
    "    if avg_class == None:\n",
    "        layer1 = tf.nn.relu(tf.matmul(input_tensor, weights1) + biases1)\n",
    "        return tf.matmul(layer1, weights2) + biases2\n",
    "    else:\n",
    "        layer1 = tf.nn.relu(tf.matmul(input_tensor, avg_class.average(weights1)) + \n",
    "                           avg_class.average(biases1))\n",
    "        return tf.matmul(layer1, avg_class.average(weights2)) + avg_class.average(biases2)\n",
    "\n",
    "# define the training process\n",
    "def train(mnist):\n",
    "    x = tf.placeholder(tf.float32, [None, INPUT_NODE], name='x-input')\n",
    "    y_ = tf.placeholder(tf.float32, [None, OUTPUT_NODE], name='y-input')\n",
    "    \n",
    "    # generate params for hidden layer\n",
    "    weights1 = tf.Variable(tf.truncated_normal([INPUT_NODE, LAYER1_NODE], stddev=0.1))\n",
    "    biases1 = tf.Variable(tf.constant(0.1, shape=[LAYER1_NODE]))\n",
    "    # generate params for output layer\n",
    "    weights2 = tf.Variable(tf.truncated_normal([LAYER1_NODE, OUTPUT_NODE], stddev=0.1))\n",
    "    biases2 = tf.Variable(tf.constant(0.1, shape=[OUTPUT_NODE]))\n",
    "    \n",
    "    # set None to not use average value of parameters\n",
    "    y = inference(x, None, weights1, biases1, weights2, biases2)\n",
    "    \n",
    "    # define the global training steps\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    # init the moving average class \n",
    "    variable_avg = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)\n",
    "    variable_avg_op = variable_avg.apply(tf.trainable_variables())\n",
    "    # use average value of parameters\n",
    "    avg_y = inference(x, variable_avg, weights1, biases1, weights2, biases2)\n",
    "    \n",
    "    # calculate the cross entropy of forecast (y) and actual (y_)\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        logits=y, labels=tf.argmax(y_, 1))\n",
    "    cross_entropy_mean = tf.reduce_mean(cross_entropy)\n",
    "    \n",
    "    # init and use regularizer function\n",
    "    regularizer = tf.contrib.layers.l2_regularizer(REGULARIZATION_RATE)\n",
    "    regularization = regularizer(weights1) + regularizer(weights2)\n",
    "    # calculate the total loss as cross entropy and reg\n",
    "    loss = cross_entropy_mean + regularization\n",
    "    \n",
    "    # define learning rate and train step\n",
    "    learning_rate = tf.train.exponential_decay(LEARNING_RATE_BASE, global_step, \n",
    "        mnist.train.num_examples, LEARNING_RATE_DECAY)\n",
    "    train_step = tf.train.GradientDescentOptimizer(learning_rate)\\\n",
    "        .minimize(loss, global_step=global_step)\n",
    "    # update the params and avg value in the same time\n",
    "    with tf.control_dependencies([train_step, variable_avg_op]):\n",
    "        train_op = tf.no_op(name='train')\n",
    "    \n",
    "    # calcuate the accuracy\n",
    "    correct_prediction = tf.equal(tf.argmax(avg_y, 1), tf.argmax(y_, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    # start the training process\n",
    "    with tf.Session() as sess:\n",
    "        tf.global_variables_initializer().run()\n",
    "        validate_feed = {x: mnist.validation.images, y_: mnist.validation.labels}\n",
    "        test_feed = {x: mnist.test.images, y_: mnist.test.labels}\n",
    "\n",
    "        for i in range(TRAINING_STEPS):\n",
    "            if i % 1000 == 0:\n",
    "                validate_acc = sess.run(accuracy, feed_dict=validate_feed)\n",
    "                print('after %d training steps, validation accuracy is %g ' %(i, validate_acc))\n",
    "            xs, ys = mnist.train.next_batch(BATCH_SIZE)\n",
    "            sess.run(train_op, feed_dict={x: xs, y_: ys})\n",
    "\n",
    "        test_acc = sess.run(accuracy, feed_dict=test_feed)\n",
    "        print('after %d training steps, test accuracy is %g ' %(TRAINING_STEPS, test_acc))\n",
    "\n",
    "# define the main function\n",
    "def main(argv=None):\n",
    "    mnist = input_data.read_data_sets('./data/mnist/', one_hot=True)\n",
    "    train(mnist)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tf.app.run()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
